[TOC]



# 概述



## 关于卡尔曼滤波

大多数现代系统都搭载上数量众多的传感器，它们通过传感器返回的一系列测量数据来估算一些有用的信息。例如，我们生活上的GPS接收器就是提供位置和速度的装置，它估算的位置和速度就是我们需要的有用数据，而不同时刻的卫星数据就是一系列的测量数据。

对于一个跟踪和控制系统来说，其中最大的问题就是在存在不确定性的前提下提供一个准确的有用信息。回到刚刚的例子，GPS接收器测量的卫星数据充满不确定性，这些不确定性往往取决于外部环境的变化，其中包括热噪声，大气层影响，卫星位置的轻微改变，GPS的内部时钟准确性等等。

而卡尔曼滤波就是众多常用且重要的估算算法。因为卡尔曼滤波器在进行预估时是默认假设输入数据是不准确的。与此同时，卡尔曼滤波是根据上一次系统的预估值来预估下一次系统的状态。

这种类型的滤波器是卡尔曼首次公开发表的，因此被命名为卡尔曼滤波器。在1960年，卡尔曼发布了一篇描述离散数据线性滤波问题的递归解的问题的论文。

现在，卡尔曼滤波器常常用于雷达跟踪系统，位置和导航系统，控制系统，计算机图形等等领域。



## 一个预测例子

在介绍卡尔曼滤波之前，让我们先来了解一下预测算法。

我们用一个雷达跟踪系统作为例子。

<img src="figures/image-20211205171335051.png" alt="image-20211205171335051" style="zoom:80%;" />
$$
\LARGE
x = x_0 + v_0\Delta t + \frac{1}{2} a\Delta t^2$\\
\large
符号说明：\\
\large
x：目标位置\\
\large
x_0：目标初始位置\\
\large 
v_0：目标初始速度\\
\large
a：目标加速度\\
\large
\Delta t：时间间隔（在本例子未5s）
$$
将上述公式映射到三维空间，我们可以将牛顿运动作为系统的方程：
$$
\LARGE
\left\{\begin{matrix} 
 x=x_0+v_{x0}+\frac{1}{2}a_x\Delta t^2\\
 y=y_0+v_{y0}+\frac{1}{2}a_y\Delta t^2\\
 z=z_0+v_{z0}+\frac{1}{2}a_z\Delta t^2
\end{matrix}\right.
$$
这些目标参数$ [x,y,z,v_x,v_y,v_z,a_x,a_y,a_z]$被称为 **系统状态**. 通过当前系统状态代入到系统方程中，我们可以得到目标的下一个系统状态。

上面的方程被称为 **动态模型** (或者**空间状态模型**). 动态模型是一种描述输入和输出关系的方法。

回到我们的例子,我们知道当我们有了当前系统状态和掌握系统的动态模型之后，我们就能很容易地预测出目标的下一个状态。

然而并不是这样的。首先,雷达系统的测量数值不是完全可靠，它包含随机误差(或者这类型的不确定性)。这些随机错误的大小取决于很多因素，例如雷达自身的准确性，发射光束的宽度，返回信号强弱等等。这些测量误差被称为**测量噪声**。

此外, 因为有很多外部因素会做成干扰，目标运动并不是完全按着运动方程。例如：风向，空气流动，驾驶策略等等。这个动态模型误差被称为 **处理噪声**。

因为测量噪声和处理噪声的存在，这个根据上诉系统方程估算出来的目标位置会真实的目标位置相差很大。假若这样，雷达系统会向错误的方向发射跟踪射束并且丢失目标。

为了提高雷达跟踪系统的表现，这就需要能够将处理噪声和测量噪声考虑进来的预测算法。

对于此类算法，应用得最广泛无疑是**卡尔曼滤波**.





# 背景知识

在正式介绍卡尔曼滤波器之前，我想先回顾一些基础概念：方差，标准差，正态分布，估计，准确度，精密度，均值，期望值和随机变量。

可能许多读者已经十分了解这些统计学基础，但我还是想确保所有读者都能掌握这部分背景知识，以便更好地理解卡尔曼滤波器的工作原理。如果您非常熟悉这些内容，可以直接跳至下一节。



## 均值和期望值

**均值 Mean** 和 **期望值 Expected Value** 是两个相似但不相同的概念。

比方说我们有5枚硬币：2枚5分和3枚10分。我们很容易就能计算出它们的均值。

![](image/卡尔曼滤波_2_01.png)
$$
\LARGE
V_{均值} = \frac{1}{N}\sum_{n=1}^{N}V_n = \frac{1}{5}(5+5+10+10+10) = 8 分
$$
上述的结果并不是期望值，因为系统的状态（硬币的值）是可观测的, 并且我们用所有个体（全部的5枚硬币）来计算均值。

现在比方说一个人测量了5次体重，5次的结果分别是：79.8kg，80kg，80.1kg，79.8kg，和80.2kg。

![](image/卡尔曼滤波_2_02.png)

体重秤的随机测量误差导致每次的测量结果不同。我们无法知道体重的真实值，因为它是一个**隐变量 Hidden Variable**，但我们可以通过平均多次测量值来估计体重。
$$
\LARGE
W=\frac{1}{N}\sum_{n=1}^{N}W_n = \frac{1}{5}(79.8+80+80.0+79.8+80.2) = 79.98kg
$$
估计的结果就叫做体重的期望值。

均值通常用希腊字母 $\mu$ 表示。

期望值通常用 $E$ 表示。



## 方差和标准差

**方差 Variance** 用来衡量一组数据的离散程度。

**标准差 Standard Deviation** 是方差的算数平方根。

标准差用希腊字母 $\sigma$ （sigma）表示，方差即为 $\sigma^2$。

假设我们想比较两所高中篮球队的身高，各队队员的身高和平均身高如下表所示。

|      | 队员 1 | 队员 2 | 队员 3 | 队员 4 | 队员 5 | 平均值 |
| :--: | :----: | :----: | :----: | :----: | :----: | :----: |
| A队  | 1.89m  |  2.1m  | 1.75m  | 1.98m  | 1.85m  | 1.914m |
| B队  | 1.94m  |  1.9m  | 1.97m  | 1.89m  | 1.87m  | 1.914m |

两队平均身高相同，我们可以进一步比较两队身高的标准差。

方差用来衡量一组数据的离散程度，即个体数据与均值的偏差。第一步，用个体值减均值来计算个体值到均值的距离。

我们用 $x$ 表示身高，希腊字母 $\mu$表示平均身高。个体值到平均值的距离为：
$$
\LARGE
x_n−\mu=x_n−1.914m
$$
下表列出了每个个体值到平均值的距离。

|      | 队员 1  | 队员 2  | 队员 3  | 队员 4  | 队员 5  |
| :--: | :-----: | :-----: | :-----: | :-----: | :-----: |
| A队  | -0.024m | 0.186m  | -0.164m | 0.066m  | -0.064m |
| B队  | 0.026m  | -0.014m | 0.056m  | -0.024m | -0.044m |

为了去掉负值，第二步，求出离均值距离的平方：
$$
\LARGE
(x_n−\mu)^2=(x_n−1.914m)^2
$$
结果如下表所示：

|      |   队员 1   |   队员 2   |   队员 3   |   队员 4   |   队员 5   |
| :--: | :--------: | :--------: | :--------: | :--------: | :--------: |
| A队  | 0.000576m2 | 0.034596m2 | 0.026896m2 | 0.004356m2 | 0.004096m2 |
| B队  | 0.000676m2 | 0.000196m2 | 0.003136m2 | 0.000576m2 | 0.001936m2 |

最后，平均各队离均值距离的平方得到方差:
$$
\LARGE
\sigma^2 = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu)^2
$$
A队的方差为：
$$
\large
σ_A^2=\frac{1}{N}\sum_{n=1}^{N}(x_n-\mu )^2(0.000576+0.034596+0.026896+0.004356+0.004096)=0.014m^2
$$
B队的方差为：
$$
\large
σ_B^2=\frac{1}{N}\sum_{n=1}^{N}(x_n-\mu )^2(0.000676+0.000196+0.003136+0.000576+0.001936)=0.013m^2
$$
虽然两支队伍的身高均值相同，但是A队的身高离散程度比B队高。这意味着A队的球员更加多样化，有不同位置的球员，比如持球手、中锋和后卫，B队就不是很全面。

由于方差的单位是平方，因此使用标准差比较方便。标准差就是方差的算数平方根。
$$
\large
\sigma = \sqrt{\frac{1}{N} \sum_{n=1}^{N} (x_n-\mu )^2}
$$
A队队员身高的标准差为0.12m。

B队队员身高的标准差为0.036m。

如果我们想计算所有高中全部队员身高的均值和方差，是非常困难的，我们需要收集所有高中球员的身高数据。

不过，我们也可以先收集一个大数据集，通过计算这个数据集来估计所有队员身高的均值和方差。

随机选取100个队员，获得的数据集足以进行准确的估计。

注意这时计算方差的方程略有不同。除数不是 $N$ , 而是 $N-1$:
$$
\large
\sigma^2 =\frac{1}{N-1}\sum_{n=1}^{N} (x_n-\mu )^2
$$
这个网站有上面公式的数学证明：http://www.visiondummy.com/2014/03/divide-variance-n-1/



## 正态分布

研究表明，许多自然现象都遵循**正态分布 Normal Distribution**。继续以篮球运动员的身高为例，如果我们随机抽取运动员建立一个大数据集，并绘制身高出现次数与身高的图表，我们会得到一条“钟形”曲线，如下图所示：

![](https://raw.githubusercontent.com/Nepqiu/gallery/master/img/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A21_03.png)

你可以看到这条曲线是沿均值1.9m对称的，并且均值附近值的出现次数高于远端值的出现次数。

这组数据的标准差为0.2m，68.26%的值位于距均值的一个标准差内。如下图所示，68.26%的值在1.7m到2.1m之间（绿色面积为曲线下总面积的68.26%）。

![](https://raw.githubusercontent.com/Nepqiu/gallery/master/img/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A21_04.png)

95.44%的值位于距均值的两个标准差内。

99.74%的值位于距均值的三个标准差内。

正态分布又名**高斯分布**（以数学家Carl Friedrich Gauss的名字命名）, 其公式如下:
$$
\LARGE
f(x;\mu ,\sigma^2) = \frac{1}{\sqrt{2\pi \sigma ^2} }e^{\frac{-(x-\mu )^2}{s\sigma ^2} }
$$
高斯分布也被称为正态分布的**概率密度函数（PDF）**。

测量误差通常呈正态分布。因此，我们在设计卡尔曼滤波器的时候假设测量误差呈正态分布。



## 随机变量

一位数学家，一位物理学家和一位工程师正在一条限速60英里/时的公路行驶时，不幸被一个拿着激光测速枪的警察拦住了。

测速枪测出车速是70英里/时，测量结果呈正态分布，标准差为5英里/时。

测速枪的测量值是一个**随机变量 Random Variable**。我们不知道的车速的精确值，只知道其**期望值 Expected Value**是70英里/时。

数学家会说，汽车的速度可以是负无穷到正无穷之间的任何数字，而速度在65到75英里/时之间的概率是68.26%。

物理学家会说，汽车的速度可以是比负光速大，比正光速小的任何数字。

工程师会说，汽车的速度可以是任何高于0低于140英里/时的数字（因为汽车朝正方向运动，而汽车的最大速度是140英里/时）。

警察会说车速是70英里/时，并开罚单。

随机变量可以是连续的，也可以是离散的:

- 电池充电时间和马拉松比赛时间是连续随机变量。
- 网站访问者的人数或班级学生的人数是离散随机变量，因为人数是可数的。

所有的测量值都是连续随机变量。



## 估计，准确度和精密度

**估计 Estimate**用来估算系统的不可见状态。飞机的真实位置对观察者来说是不可见的。我们可以用雷达等传感器估计飞机的位置，并通过使用多个传感器和高级估计及追踪算法（如卡尔曼滤波器）来显著地提升估计的准确度。测量或计算出的的参数都是估计值。

**准确度 Accuracy**表示测量值与真实值的接近程度。

**精密度 Precision**表示测量结果的再现性。估计需要考虑系统的准确性与精密性。

下图说明了准确度和精密度的关系。

![](https://raw.githubusercontent.com/Nepqiu/gallery/master/img/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A21_05.png)

随机测量误差会导致方差，高精密度系统的方差较小（不确定度低），而低精密度系统的方差较大（不确定度高）。

低精度系统被称为**有偏 biased**系统，因为其测量值都存在一个固有的系统性误差（偏差）。

对测量值进行平均或平滑处理可以显著降低方差的影响。例如，如果我们使用带有随机测量误差的温度计来测量温度，随机的误差将导致一些测量值高于真实值，一些低于真实值。我们可以进行多次测量并对其求平均值，这个估计值会接近真实值，测量次数越多，估计值就越接近。

但如果温度计有偏差，估计值会有一个固定的系统误差。

本教程中的所有例子都假设为**无偏 unbiased**系统。



## 小结

下图为测量值的统计学表达。

![](https://raw.githubusercontent.com/Nepqiu/gallery/master/img/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A21_06.png)

测量值是一个由**概率密度函数 Probability Density Function (PDF)**描述的**随机变量 random variable**。

测量值的均值即为随机变量的**期望值 Expected Value**。

测量值的均值和真实值的差名为**偏差 bias** 或 **系统误差 systematic measurement error**，用来表明**测量精度 measurements accuracy**。

分布的离散程度就是测量值的**精密度 precision**，又名**测量噪声 measurement noise**或**随机测量误差 random measurement error**或**测量不确定性 measurement uncertainty**.





# 递归算法

>  最优化 递归 数字处理 算法

$$
\large
\begin{eqnarray}    \label{eq}
\hat{x_k} &=& \frac{1}{k} (z_1 + z_2 + z_3 + ... + z_k) \\
&=& \frac{1}{k} (z_1 + z_2 + z_3 + ... + z_{k-1}) + \frac{1}{k} z_k \\
&=& \frac{1}{k} \frac{k-1}{\color{#0000FF}{k-1}} {\color{blue} (z_1 + z_2 + z_3 + ... + z_{k-1})}+ \frac{1}{k} z_k \\
&=& \frac{k-1}{k} \hat{x_{k-1}} + \frac{1}{k} z_k \\
&=& \hat{x_{k-1}} - \frac{1}{k} \hat{x_{k-1}} + \frac{1}{k} z_k \\
\\
\color{red}
\hat{x_k} &\color{red}=& \color{red}\hat{x_{k-1}} + \frac{1}{k}(z_k - \hat{x_{k-1}})
\end{eqnarray}
$$

${\color{blue} \frac{(z_1 + z_2 + z_3 + ... + z_{k-1})}{k-1}}$：也就是$\hat{x_{k-1}}$是上一次的估计值

随着$k$的增加，$\frac{1}{k}$趋近于0，$\hat{x_k} -> \hat{x_{k-1}}$也就是说随着测量的增加，测量的结果也不再重要了（对估计值有一定信心了）

$k$比较小的时候，$\frac{1}{k}$比较大，所以$z_k(测量结果)$的作用也比较大

$$
\LARGE
\begin{eqnarray}
\hat{x_k} &=& \hat{x_{k-1}} + \frac{1}{k}(z_k - \hat{x_{k-1}}) \\
令 \frac{1}{k} &=& k_k \\
当前的估计值 &=& 上一次的估计值 + 系数 * （当前测量值 - 上一次的估计值） \\
k_k &=& Kalman Gain - 卡尔曼增益/系数
\end{eqnarray}
$$
不需要追溯很久以前的数据，只需要知道上一次的就可以了


$$
\LARGE
估计误差: {\color{red}{e_{EST}}} = (1-k_k)e_{EST_{k-1}} （error：误差  Estimate：估计） \\
\LARGE
测量误差: {\color{red}{e_{MEA}}}  （measurement：测量）\\
\LARGE
卡尔曼增益：{\color{red}k_k} = \frac{e_{EST_{k-1}}}{e_{EST_{k-1}} + e_{MEA}}
$$

- 当$e_{EST_{k-1}} \gg e_{MEA}$: $k_k \to 1$
  $$
  \LARGE
  \hat{x_k} = \hat{x_{k-1}} + z_k - \hat{x_{k-1}} \approx z_k
  $$
  
- 当$e_{EST_{k-1}} \ll e_{MEA}$: $k_k \to 0$
  $$
  \LARGE
  \hat{x_k} \approx \hat{x_{k-1}}
  $$









